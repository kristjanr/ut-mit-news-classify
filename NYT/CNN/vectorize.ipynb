{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Xi61aWUbEhcK"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NUSrOjMIJlN4"
   },
   "outputs": [],
   "source": [
    "PAD = '<PAD>'\n",
    "PAD_ID = 0\n",
    "UNK = '<UNK>'\n",
    "UNK_ID = 1\n",
    "VOCAB_PREFIX = [PAD, UNK]\n",
    "\n",
    "VEC_PATH = Path('wiki-news-300d-1M.vec')\n",
    "DATA_PATH = Path('.')\n",
    "MAX_VOCAB = 25000\n",
    "\n",
    "batch_size = 64\n",
    "validation_split = .3\n",
    "shuffle_dataset = True\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oJx488cqJlPT"
   },
   "outputs": [],
   "source": [
    "class BaseVocab:\n",
    "    def __init__(self, data, lower=False):\n",
    "        self.data = data\n",
    "        self.lower = lower\n",
    "        self.build_vocab()\n",
    "        \n",
    "    def normalize_unit(self, unit):\n",
    "        if self.lower:\n",
    "            return unit.lower()\n",
    "        else:\n",
    "            return unit\n",
    "        \n",
    "    def unit2id(self, unit):\n",
    "        unit = self.normalize_unit(unit)\n",
    "        if unit in self._unit2id:\n",
    "            return self._unit2id[unit]\n",
    "        else:\n",
    "            return self._unit2id[UNK]\n",
    "    \n",
    "    def id2unit(self, id):\n",
    "        return self._id2unit[id]\n",
    "    \n",
    "    def map(self, units):\n",
    "        return [self.unit2id(unit) for unit in units]\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        NotImplementedError()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._unit2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QY-QgWo9JlTj"
   },
   "outputs": [],
   "source": [
    "class PretrainedWordVocab(BaseVocab):\n",
    "    def build_vocab(self):\n",
    "        self._id2unit = VOCAB_PREFIX + self.data\n",
    "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ar7b46jYJlXC"
   },
   "outputs": [],
   "source": [
    "class LabelVocab(BaseVocab):\n",
    "    def build_vocab(self):\n",
    "        self._id2unit = self.data\n",
    "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NBByipT4L4Kw"
   },
   "outputs": [],
   "source": [
    "class Pretrain:\n",
    "    def __init__(self, vec_filename, max_vocab=-1):\n",
    "        self._vec_filename = vec_filename\n",
    "        self._max_vocab = max_vocab\n",
    "        \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        if not hasattr(self, '_vocab'):\n",
    "            self._vocab, self._emb = self.read()\n",
    "        return self._vocab\n",
    "    \n",
    "    @property\n",
    "    def emb(self):\n",
    "        if not hasattr(self, '_emb'):\n",
    "            self._vocab, self._emb = self.read()\n",
    "        return self._emb\n",
    "        \n",
    "    def read(self):\n",
    "        if self._vec_filename is None:\n",
    "            raise Exception(\"Vector file is not provided.\")\n",
    "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
    "        \n",
    "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
    "        \n",
    "        if failed > 0: # recover failure\n",
    "            emb = emb[:-failed]\n",
    "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
    "            raise Exception(\"Loaded number of vectors does not match number of words.\")\n",
    "            \n",
    "        # Use a fixed vocab size\n",
    "        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n",
    "            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n",
    "            emb = emb[:self._max_vocab]\n",
    "                \n",
    "        vocab = PretrainedWordVocab(words, lower=True)\n",
    "        print(\"Done Reading\")\n",
    "        \n",
    "        return vocab, emb\n",
    "        \n",
    "    def read_from_file(self, filename, open_func=open):\n",
    "        \"\"\"\n",
    "        Open a vector file using the provided function and read from it.\n",
    "        \"\"\"\n",
    "        first = True\n",
    "        words = []\n",
    "        failed = 0\n",
    "        with open_func(filename, 'rb') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                try:\n",
    "                    line = line.decode()\n",
    "                except UnicodeDecodeError:\n",
    "                    failed += 1\n",
    "                    continue\n",
    "                if first:\n",
    "                    # the first line contains the number of word vectors and the dimensionality\n",
    "                    first = False\n",
    "                    line = line.strip().split(' ')\n",
    "                    rows, cols = [int(x) for x in line]\n",
    "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
    "                    continue\n",
    "\n",
    "                line = line.rstrip().split(' ')\n",
    "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
    "                words.append(' '.join(line[:-cols]))\n",
    "        return words, emb, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Kb7sjmY2nrNv"
   },
   "outputs": [],
   "source": [
    "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5e0JyfjYL71w"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Vectorizer():\n",
    "    def __init__(self, pretrain, data, tokenzier, label_vocab):\n",
    "        self.pretrain_vocab = pretrain.vocab\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_vocab = label_vocab\n",
    "        \n",
    "\n",
    "    def vectorize(self):\n",
    "        articles = [a[2] for a in self.data]\n",
    "        labels = [a[3:] for a in self.data]\n",
    "         \n",
    "        vectorized_data = []\n",
    "     \n",
    "        for article, label_words in zip(articles, labels):\n",
    "            tokens = [t.text for t in self.tokenizer(article)]\n",
    "            text = torch.LongTensor(self.pretrain_vocab.map(tokens))\n",
    "\n",
    "            label_indices = torch.LongTensor(self.label_vocab.map(label_words))\n",
    "\n",
    "            n_labels = len(self.label_vocab)\n",
    "            src = torch.ones(n_labels)\n",
    "            label = torch.zeros(n_labels).scatter_(0, label_indices, src)\n",
    "            label = torch.FloatTensor(label)\n",
    "\n",
    "            vectorized_data.append((text, label))\n",
    "            \n",
    "        vectorized_data = sorted(vectorized_data, key=lambda x: len(x[0]), reverse=True)\n",
    "        \n",
    "        return vectorized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mY0-pAwpEn4B"
   },
   "outputs": [],
   "source": [
    "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3nsAvXhZjkZ",
    "outputId": "989b8fab-848e-4d89-bf7a-4e5f32e0c961"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check if we are running on a CPU or GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYDGJBkvk7gN",
    "outputId": "04f32f90-dcac-4214-f90b-1a66fbf543d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.27 s, sys: 8.34 s, total: 16.6 s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "training_data_path = DATA_PATH / 'NYTcorpus_train.p'\n",
    "with open(training_data_path, mode='rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def get_labels(data):\n",
    "    labels = [a[3:] for a in data]\n",
    "    labels_flattened = []\n",
    "    for label in labels:\n",
    "        labels_flattened.extend(label)\n",
    "    return LabelVocab(list(set(labels_flattened)))\n",
    "    \n",
    "label_vocab = get_labels(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3IL9y07E1mj",
    "outputId": "68a7b282-316a-4160-f58d-1e0bf844a1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 train vect time taken: 141 seconds\n",
      "13 train vect time taken: 145 seconds\n",
      "14 train vect time taken: 123 seconds\n",
      "15 train vect time taken: 133 seconds\n",
      "16 train vect time taken: 120 seconds\n",
      "17 train vect time taken: 130 seconds\n",
      "18 train vect time taken: 123 seconds\n",
      "19 train vect time taken: 126 seconds\n",
      "CPU times: user 15min 58s, sys: 41.7 s, total: 16min 40s\n",
      "Wall time: 17min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "#random.Random(0).shuffle(data)\n",
    "\n",
    "\n",
    "nlp = English()\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "\n",
    "for i, data_to_vectorize in enumerate(np.array_split(data, 20)):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    train_vectorizer = Vectorizer(pretrain, data_to_vectorize, tokenizer, label_vocab)\n",
    "\n",
    "    vectorized_data = train_vectorizer.vectorize()\n",
    "\n",
    "    torch.save(vectorized_data, f'all_train_vect/{i}_{len(vectorized_data)}.pt')\n",
    "\n",
    "    print(f\"{i} train vect time taken: {int(time.time() - start)} seconds\")\n",
    "    \n",
    "del data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 608 ms, sys: 377 ms, total: 985 ms\n",
      "Wall time: 994 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "test_data_path = DATA_PATH / 'NYTcorpus_test.p'\n",
    "with open(test_data_path, mode='rb') as f:\n",
    "    test_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test vect time taken: 287 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "test_vectorizer = Vectorizer(pretrain, test_data, tokenizer, label_vocab)\n",
    "\n",
    "vectorized_data = test_vectorizer.vectorize()\n",
    "\n",
    "torch.save(vectorized_data, f'all_test_vect/all_{len(vectorized_data)}.pt')\n",
    "print(f\"test vect time taken: {int(time.time() - start)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NYT-CNN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
