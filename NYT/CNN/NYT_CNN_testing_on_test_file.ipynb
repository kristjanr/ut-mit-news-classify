{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NYT-CNN testing on test file",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COl-b3Cuj_af",
        "outputId": "cf8ee2fe-08f3-40c3-bdc3-96b710e9744f"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/5a/b037b50f9849212863a2fed313624d8f6f33ffa4ce89dc706e2a0e98c780/wandb-0.10.29-py2.py3-none-any.whl (2.1MB)\n",
            "\r\u001b[K     |▏                               | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 18.1MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 15.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 14.1MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 9.5MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 8.7MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 9.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 10.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 92kB 10.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▉                             | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |████                            | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |████                            | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 317kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 327kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 337kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 348kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 358kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 368kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 378kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 389kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 399kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 409kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 419kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 430kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 440kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 450kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 460kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 471kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 481kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 491kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 501kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 512kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 522kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 532kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 542kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 552kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 563kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 573kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 583kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 593kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 604kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 614kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 624kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 634kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 645kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 655kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 665kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 675kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 686kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 696kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 706kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 716kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 727kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 737kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 747kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 757kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 768kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 778kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 788kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 798kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 808kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 819kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 829kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 839kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 849kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 860kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 870kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 880kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 890kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 901kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 911kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 921kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 931kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 942kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 952kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 962kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 972kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 983kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 993kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.2MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.3MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.3MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.3MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.3MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.4MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.4MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.4MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.4MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.5MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.5MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.5MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.5MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.5MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.5MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.5MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.5MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.6MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.6MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.6MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.6MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.6MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.6MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.6MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.7MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.7MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.7MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.7MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.7MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.7MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.7MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.8MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.8MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.8MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.8MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.8MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.8MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.8MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.8MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.9MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.9MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.9MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.9MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.9MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.9MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.9MB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.9MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.0MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.1MB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 56.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 48.7MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.8MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.0.0)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=7d71c2554e0aa0248f1a30cd0011b48591048c0f512137d199e9a4c66d87bd9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=855fd8829d184f2bb2041f972026536113691fb66498201e1e12ad025fb5dca6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: sentry-sdk, smmap, gitdb, GitPython, shortuuid, subprocess32, docker-pycreds, configparser, pathtools, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwSxA1lw40sS",
        "outputId": "2f17e5df-b2ad-4b3f-eb67-ea7a2de977da"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May  7 09:41:44 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgj88POfmDYv",
        "outputId": "7aec4b79-49ab-4b81-8d8a-3bd3ab08d10d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/NLP/project/\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/NLP/project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi61aWUbEhcK"
      },
      "source": [
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import wandb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUSrOjMIJlN4"
      },
      "source": [
        "PAD = '<PAD>'\n",
        "PAD_ID = 0\n",
        "UNK = '<UNK>'\n",
        "UNK_ID = 1\n",
        "VOCAB_PREFIX = [PAD, UNK]\n",
        "\n",
        "VEC_PATH = Path('vector_cache') / 'wiki-news-300d-1M.vec'\n",
        "MAX_VOCAB = 25000\n",
        "\n",
        "batch_size = 64\n",
        "validation_split = .3\n",
        "shuffle_dataset = True\n",
        "random_seed = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJx488cqJlPT"
      },
      "source": [
        "class BaseVocab:\n",
        "    def __init__(self, data, lower=False):\n",
        "        self.data = data\n",
        "        self.lower = lower\n",
        "        self.build_vocab()\n",
        "        \n",
        "    def normalize_unit(self, unit):\n",
        "        if self.lower:\n",
        "            return unit.lower()\n",
        "        else:\n",
        "            return unit\n",
        "        \n",
        "    def unit2id(self, unit):\n",
        "        unit = self.normalize_unit(unit)\n",
        "        if unit in self._unit2id:\n",
        "            return self._unit2id[unit]\n",
        "        else:\n",
        "            return self._unit2id[UNK]\n",
        "    \n",
        "    def id2unit(self, id):\n",
        "        return self._id2unit[id]\n",
        "    \n",
        "    def map(self, units):\n",
        "        return [self.unit2id(unit) for unit in units]\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._unit2id)\n",
        "\n",
        "\n",
        "class PretrainedWordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = VOCAB_PREFIX + self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}\n",
        "\n",
        "\n",
        "class LabelVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}\n",
        "\n",
        "\n",
        "class Pretrain:\n",
        "    def __init__(self, vec_filename, max_vocab=-1):\n",
        "        self._vec_filename = vec_filename\n",
        "        self._max_vocab = max_vocab\n",
        "        \n",
        "    @property\n",
        "    def vocab(self):\n",
        "        if not hasattr(self, '_vocab'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._vocab\n",
        "    \n",
        "    @property\n",
        "    def emb(self):\n",
        "        if not hasattr(self, '_emb'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._emb\n",
        "        \n",
        "    def read(self):\n",
        "        if self._vec_filename is None:\n",
        "            raise Exception(\"Vector file is not provided.\")\n",
        "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
        "        \n",
        "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
        "        \n",
        "        if failed > 0: # recover failure\n",
        "            emb = emb[:-failed]\n",
        "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
        "            raise Exception(\"Loaded number of vectors does not match number of words.\")\n",
        "            \n",
        "        # Use a fixed vocab size\n",
        "        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n",
        "            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n",
        "            emb = emb[:self._max_vocab]\n",
        "                \n",
        "        vocab = PretrainedWordVocab(words, lower=True)\n",
        "        print(\"Done Reading\")\n",
        "        \n",
        "        return vocab, emb\n",
        "        \n",
        "    def read_from_file(self, filename, open_func=open):\n",
        "        \"\"\"\n",
        "        Open a vector file using the provided function and read from it.\n",
        "        \"\"\"\n",
        "        first = True\n",
        "        words = []\n",
        "        failed = 0\n",
        "        with open_func(filename, 'rb') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    line = line.decode()\n",
        "                except UnicodeDecodeError:\n",
        "                    failed += 1\n",
        "                    continue\n",
        "                if first:\n",
        "                    # the first line contains the number of word vectors and the dimensionality\n",
        "                    first = False\n",
        "                    line = line.strip().split(' ')\n",
        "                    rows, cols = [int(x) for x in line]\n",
        "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
        "                    continue\n",
        "\n",
        "                line = line.rstrip().split(' ')\n",
        "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
        "                words.append(' '.join(line[:-cols]))\n",
        "        return words, emb, failed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI3qkifadUn5"
      },
      "source": [
        "def pad_sequences(batch):\n",
        "    max_len = max([len(x[0]) for x in batch])\n",
        "    padded_sequences = torch.zeros((len(batch), max_len), dtype=torch.long)\n",
        "    labels = torch.zeros((len(batch), len(batch[0][1])), dtype=torch.float)\n",
        "    for i, sample in enumerate(batch):\n",
        "\n",
        "      padded_sequences[i, :len(sample[0])] = sample[0]\n",
        "      labels[i, :] = sample[1]\n",
        "\n",
        "    padded_sequences = padded_sequences.to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    return padded_sequences, labels\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, pretrain, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "                \n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.from_numpy(pretrain.emb), \n",
        "            padding_idx=pad_idx, \n",
        "            freeze=True\n",
        "        )\n",
        "        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):           \n",
        "        #text = [batch size, sent len]\n",
        "\n",
        "        embedded = self.embedding(text)     \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)  \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]    \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "                \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]     \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def multi_label_scores(correct_labels, predicted_labels):\n",
        "    \n",
        "    predicted_labels = torch.round(torch.sigmoid(predicted_labels.cpu())).detach().numpy()\n",
        "    correct_labels = correct_labels.cpu().detach().numpy()\n",
        "\n",
        "    accuracy = accuracy_score(correct_labels, predicted_labels)\n",
        "    precision = precision_score(correct_labels, predicted_labels, average='weighted', zero_division=0)\n",
        "    recall = recall_score(correct_labels, predicted_labels, average='weighted', zero_division=0)\n",
        "    f_1_score = f1_score(correct_labels, predicted_labels, average='weighted', zero_division=0)\n",
        "    \n",
        "    return accuracy, precision, recall, f_1_score\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f_score = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch[0]).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch[1])\n",
        "      \n",
        "            acc, precision, recall, f_score = multi_label_scores(batch[1], predictions)\n",
        "\n",
        "            epoch_loss += loss\n",
        "            epoch_acc += acc\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f_score += f_score\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), \\\n",
        "        epoch_precision / len(iterator), epoch_recall / len(iterator), \\\n",
        "        epoch_f_score / len(iterator)\n",
        "        \n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time \n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e0JyfjYL71w"
      },
      "source": [
        "\n",
        "class NYTDataSet(Dataset):\n",
        "    def __init__(self, vectorized_data):\n",
        "        self.data = vectorized_data\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY0-pAwpEn4B"
      },
      "source": [
        "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3nsAvXhZjkZ",
        "outputId": "68da5725-96a5-43f8-9220-13b6ed4785b6"
      },
      "source": [
        "\n",
        "# Check if we are running on a CPU or GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfVSEjTa6T0T"
      },
      "source": [
        "def load(file_path):\n",
        "    vectorized_data = torch.load(file_path)\n",
        "    dataset = NYTDataSet(vectorized_data=vectorized_data)\n",
        "\n",
        "    # Creating data indices for training and validation splits:\n",
        "    dataset_size = len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    split = int(np.floor(validation_split * dataset_size))\n",
        "    if shuffle_dataset:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "    train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "    # Creating PT data samplers and loaders:\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=pad_sequences)\n",
        "    validation_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, collate_fn=pad_sequences)\n",
        "    return dataset_size, train_loader, validation_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAMomkn98SE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472f5a5b-9a4c-41c1-e30c-b85f0ea6ef72"
      },
      "source": [
        "INPUT_DIM = len(pretrain.vocab)\n",
        "EMBEDDING_DIM = pretrain.emb.shape[1]\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5,6]\n",
        "OUTPUT_DIM = 538\n",
        "DROPOUT = 0.6\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading pretrained vectors from vector_cache/wiki-news-300d-1M.vec...\n",
            "Done Reading\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YpSzzzt-3Y6"
      },
      "source": [
        "def load_test(file_path):\n",
        "    vectorized_test_data = torch.load(file_path)\n",
        "    test_data = NYTDataSet(vectorized_data=vectorized_test_data)\n",
        "    return DataLoader(test_data, batch_size=batch_size, collate_fn=pad_sequences)\n",
        "    \n",
        "start_time = time.time()\n",
        "\n",
        "test_file = 'all_test_vect/all_144279.pt'\n",
        "\n",
        "test_loader = load_test(test_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F72a1UaXY0iJ"
      },
      "source": [
        "#    def __init__(self, pretrain, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "#                  dropout, pad_idx):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_QHGnca6ftR",
        "outputId": "88338594-9347-49f0-b4c5-f6c1056e6785"
      },
      "source": [
        "\n",
        "model_file_name = 'nyt_cnn_classifier_all_train_vect.pt'\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = CNN(pretrain, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_ID)\n",
        "model.load_state_dict(torch.load(model_file_name))\n",
        "model = model.to(device)\n",
        "\n",
        "test_loss, test_acc, test_precision, test_recall, test_f_score = evaluate(model, test_loader, criterion)\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "print(f'Epoch: test | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | ' +\n",
        "      f'Test Precision: {test_precision*100:.2f}% | Test Recall: {test_recall*100:.2f}% | ' +\n",
        "      f'Test F1-score: {test_f_score*100:.2f}%')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: test | Epoch Time: 3m 25s\n",
            "\tTest Loss: 0.015 | Test Acc: 26.30% | Test Precision: 48.83% | Test Recall: 37.38% | Test F1-score: 40.68%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}