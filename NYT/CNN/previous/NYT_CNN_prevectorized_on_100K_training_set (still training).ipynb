{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NYT-CNN prevectorized on 100K training set",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9e290831c8d4909b93c1526af2e8f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e715548daaef4d83a08b7969edaab5fb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a60635c6077e4263baff0d2e3d79b5ba",
              "IPY_MODEL_e9da4f8cb829405cb59261244c80e348"
            ]
          }
        },
        "e715548daaef4d83a08b7969edaab5fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a60635c6077e4263baff0d2e3d79b5ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_e68ac396ef534e178835b1e8e100a49d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.03MB of 0.03MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38d30b86aa5f45368186f09a863fab3d"
          }
        },
        "e9da4f8cb829405cb59261244c80e348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d8031b93aed04f429321f9b8d2a49a5f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19f44bcc005d48e0baaa8ac5500e8023"
          }
        },
        "e68ac396ef534e178835b1e8e100a49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38d30b86aa5f45368186f09a863fab3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d8031b93aed04f429321f9b8d2a49a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19f44bcc005d48e0baaa8ac5500e8023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COl-b3Cuj_af",
        "outputId": "989b81bf-900a-4d5e-a6bb-c49faf7fc3d4"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/28/4aefc543967839bdb4e139831b82004279f1c435cede2a9557ccf8369875/wandb-0.10.27-py2.py3-none-any.whl (2.1MB)\n",
            "\r\u001b[K     |▏                               | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 16.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 13.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 12.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 9.0MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 8.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 9.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 92kB 9.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 143kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 163kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 174kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 184kB 8.6MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 204kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 215kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 225kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 235kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 245kB 8.6MB/s eta 0:00:01\r\u001b[K     |████                            | 256kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 266kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 276kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 286kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 296kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 307kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 317kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 327kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 337kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 348kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 358kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 368kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 378kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 389kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 399kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 409kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 419kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 430kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 440kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 450kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 460kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 471kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 481kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 491kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 501kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 512kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 522kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 532kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 542kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 552kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 563kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 573kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 583kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 593kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 604kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 614kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 624kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 634kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 645kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 655kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 665kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 675kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 686kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 696kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 706kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 716kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 727kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 737kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 747kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 757kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 768kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 778kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 788kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 798kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 808kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 819kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 829kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 839kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 849kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 860kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 870kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 880kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 890kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 901kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 911kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 921kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 931kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 942kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 952kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 962kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 972kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 983kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 993kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 33.2MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.7MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=1f4cafe20e5d0368a930987c430670a4a0962747237a2164f7d661111d3dcb15\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=289097ab8a2a063e5ad2390829372137c98feecfc6435fb6d68f93eac0a57b84\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: docker-pycreds, configparser, sentry-sdk, smmap, gitdb, GitPython, subprocess32, shortuuid, pathtools, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi61aWUbEhcK"
      },
      "source": [
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from spacy.lang.en import English\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import wandb\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGaDt3jDFoYt",
        "outputId": "ae9e8047-62c1-4b6c-9d3a-a91d056cd949"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/NLP/project/\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/NLP/project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRTTJd9_HVzi"
      },
      "source": [
        "# %mkdir NYTData\n",
        "# !wget https://www.dropbox.com/sh/xu9tu5hmjhuddwk/AAD31tK6oEoGlhpRZzeu3Y3Ya/NYTcorpus_train.p.gz?dl=1 --directory-prefix=NYTData\n",
        "# !gunzip -c NYTData/NYTcorpus_train.p.gz\\?dl\\=1 > NYTData/NYTcorpus_train.p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvyDYMH_JlHR"
      },
      "source": [
        "# %mkdir vector_cache\n",
        "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip --directory-prefix=vector_cache\n",
        "# !unzip vector_cache/wiki-news-300d-1M.vec.zip -d vector_cache/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUSrOjMIJlN4"
      },
      "source": [
        "PAD = '<PAD>'\n",
        "PAD_ID = 0\n",
        "UNK = '<UNK>'\n",
        "UNK_ID = 1\n",
        "VOCAB_PREFIX = [PAD, UNK]\n",
        "\n",
        "VEC_PATH = Path('vector_cache') / 'wiki-news-300d-1M.vec'\n",
        "DATA_PATH = Path('NYTData')\n",
        "MAX_VOCAB = 25000\n",
        "\n",
        "batch_size = 64\n",
        "validation_split = .3\n",
        "shuffle_dataset = True\n",
        "random_seed = 42"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJx488cqJlPT"
      },
      "source": [
        "class BaseVocab:\n",
        "    def __init__(self, data, lower=False):\n",
        "        self.data = data\n",
        "        self.lower = lower\n",
        "        self.build_vocab()\n",
        "        \n",
        "    def normalize_unit(self, unit):\n",
        "        if self.lower:\n",
        "            return unit.lower()\n",
        "        else:\n",
        "            return unit\n",
        "        \n",
        "    def unit2id(self, unit):\n",
        "        unit = self.normalize_unit(unit)\n",
        "        if unit in self._unit2id:\n",
        "            return self._unit2id[unit]\n",
        "        else:\n",
        "            return self._unit2id[UNK]\n",
        "    \n",
        "    def id2unit(self, id):\n",
        "        return self._id2unit[id]\n",
        "    \n",
        "    def map(self, units):\n",
        "        return [self.unit2id(unit) for unit in units]\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._unit2id)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY-QgWo9JlTj"
      },
      "source": [
        "class PretrainedWordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = VOCAB_PREFIX + self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar7b46jYJlXC"
      },
      "source": [
        "class LabelVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBByipT4L4Kw"
      },
      "source": [
        "class Pretrain:\n",
        "    def __init__(self, vec_filename, max_vocab=-1):\n",
        "        self._vec_filename = vec_filename\n",
        "        self._max_vocab = max_vocab\n",
        "        \n",
        "    @property\n",
        "    def vocab(self):\n",
        "        if not hasattr(self, '_vocab'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._vocab\n",
        "    \n",
        "    @property\n",
        "    def emb(self):\n",
        "        if not hasattr(self, '_emb'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._emb\n",
        "        \n",
        "    def read(self):\n",
        "        if self._vec_filename is None:\n",
        "            raise Exception(\"Vector file is not provided.\")\n",
        "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
        "        \n",
        "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
        "        \n",
        "        if failed > 0: # recover failure\n",
        "            emb = emb[:-failed]\n",
        "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
        "            raise Exception(\"Loaded number of vectors does not match number of words.\")\n",
        "            \n",
        "        # Use a fixed vocab size\n",
        "        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n",
        "            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n",
        "            emb = emb[:self._max_vocab]\n",
        "                \n",
        "        vocab = PretrainedWordVocab(words, lower=True)\n",
        "        print(\"Done Reading\")\n",
        "        \n",
        "        return vocab, emb\n",
        "        \n",
        "    def read_from_file(self, filename, open_func=open):\n",
        "        \"\"\"\n",
        "        Open a vector file using the provided function and read from it.\n",
        "        \"\"\"\n",
        "        first = True\n",
        "        words = []\n",
        "        failed = 0\n",
        "        with open_func(filename, 'rb') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    line = line.decode()\n",
        "                except UnicodeDecodeError:\n",
        "                    failed += 1\n",
        "                    continue\n",
        "                if first:\n",
        "                    # the first line contains the number of word vectors and the dimensionality\n",
        "                    first = False\n",
        "                    line = line.strip().split(' ')\n",
        "                    rows, cols = [int(x) for x in line]\n",
        "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
        "                    continue\n",
        "\n",
        "                line = line.rstrip().split(' ')\n",
        "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
        "                words.append(' '.join(line[:-cols]))\n",
        "        return words, emb, failed"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e0JyfjYL71w"
      },
      "source": [
        "\n",
        "class NYTDataSet(Dataset):\n",
        "    def __init__(self, vectorized_data):\n",
        "        self.data = vectorized_data\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkviEhLmDkf6"
      },
      "source": [
        "def pad_sequences(batch):\n",
        "    max_len = max([len(x[0]) for x in batch])\n",
        "    padded_sequences = torch.zeros((len(batch), max_len), dtype=torch.long)\n",
        "    labels = torch.zeros((len(batch), len(batch[0][1])), dtype=torch.float)\n",
        "    for i, sample in enumerate(batch):\n",
        "\n",
        "      padded_sequences[i, :len(sample[0])] = sample[0]\n",
        "      labels[i, :] = sample[1]\n",
        "\n",
        "    padded_sequences = padded_sequences.to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    return padded_sequences, labels"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY0-pAwpEn4B"
      },
      "source": [
        "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3nsAvXhZjkZ",
        "outputId": "de86a507-b10c-480a-b9fa-ff72a61d366e"
      },
      "source": [
        "\n",
        "nlp = English()\n",
        "tokenizer = nlp.tokenizer\n",
        "\n",
        "# Check if we are running on a CPU or GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYDGJBkvk7gN",
        "outputId": "6ac0991e-b9e4-406f-fbe5-41a68cd18252"
      },
      "source": [
        "%%time \n",
        "vectorized_train_data = torch.load('vectorized_100000-10000_train.pt')\n",
        "train_data = NYTDataSet(vectorized_data=vectorized_train_data)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8.96 s, sys: 3.22 s, total: 12.2 s\n",
            "Wall time: 22.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8i8vaorcwan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e048aee8-14a0-4efe-86d9-ba3e64e26df2"
      },
      "source": [
        "%%time\n",
        "vectorized_test_data = torch.load('vectorized_100000-10000_test.pt')\n",
        "test_data = NYTDataSet(vectorized_data=vectorized_test_data)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.69 s, sys: 323 ms, total: 2.01 s\n",
            "Wall time: 3.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j_JvLHGFtT4"
      },
      "source": [
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset:\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8NcPUf6dMz6"
      },
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, collate_fn=pad_sequences)\n",
        "validation_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, collate_fn=pad_sequences)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=pad_sequences)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI3qkifadUn5"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, pretrain, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "                \n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.from_numpy(pretrain.emb), \n",
        "            padding_idx=pad_idx, \n",
        "            freeze=True\n",
        "        )\n",
        "        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):           \n",
        "        #text = [batch size, sent len]\n",
        "\n",
        "        embedded = self.embedding(text)     \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)  \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]    \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "                \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]     \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJp7AOCydb6C"
      },
      "source": [
        "INPUT_DIM = len(pretrain.vocab)\n",
        "EMBEDDING_DIM = pretrain.emb.shape[1]\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5,6]\n",
        "OUTPUT_DIM = 538\n",
        "DROPOUT = 0.6\n",
        "\n",
        "model = CNN(pretrain, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_ID)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYYEkiO8dm7M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f9e290831c8d4909b93c1526af2e8f75",
            "e715548daaef4d83a08b7969edaab5fb",
            "a60635c6077e4263baff0d2e3d79b5ba",
            "e9da4f8cb829405cb59261244c80e348",
            "e68ac396ef534e178835b1e8e100a49d",
            "38d30b86aa5f45368186f09a863fab3d",
            "d8031b93aed04f429321f9b8d2a49a5f",
            "19f44bcc005d48e0baaa8ac5500e8023"
          ]
        },
        "outputId": "c4c2e17c-809c-4652-8a6b-daa16c892b23"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "wandb.init(\n",
        "  project=\"NYT Multilabeling\",\n",
        ")\n",
        "# Magic\n",
        "wandb.watch(model)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:2gcsfx42) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1274<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9e290831c8d4909b93c1526af2e8f75",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/drive/My Drive/Colab Notebooks/NLP/project/wandb/run-20210424_175940-2gcsfx42/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/drive/My Drive/Colab Notebooks/NLP/project/wandb/run-20210424_175940-2gcsfx42/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>train_loss</td><td>0.01392</td></tr><tr><td>train_precision</td><td>0.80448</td></tr><tr><td>train_f_score</td><td>0.51543</td></tr><tr><td>train_acc</td><td>0.99597</td></tr><tr><td>train_recall</td><td>0.38165</td></tr><tr><td>valid_loss</td><td>0.02155</td></tr><tr><td>valid_acc</td><td>0.99516</td></tr><tr><td>valid_precision</td><td>0.85882</td></tr><tr><td>valid_recall</td><td>0.20307</td></tr><tr><td>valid_f_score</td><td>0.32664</td></tr><tr><td>_runtime</td><td>669</td></tr><tr><td>_timestamp</td><td>1619287852</td></tr><tr><td>_step</td><td>42</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>train_loss</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_precision</td><td>    █▅▃▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▃▂▃▃▃▃▃▃▄▃▄▄▄▄▄▄▄▄</td></tr><tr><td>train_f_score</td><td>    ▁▁▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>train_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>train_recall</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>valid_loss</td><td>██▇▇▆▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>valid_acc</td><td>▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███████████</td></tr><tr><td>valid_precision</td><td>   █ ▅▄▄▂▂▄▄▃▂▃▂▂▃▂▃▃▁▂▂▂▁▂▂▂▂▂▁▁▁▃▁▃▂▃▃</td></tr><tr><td>valid_recall</td><td>▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▆▇▇▇▇▇▇███████▇███</td></tr><tr><td>valid_f_score</td><td>   ▁ ▂▂▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▆▇▇▇▇▇▇███████████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">glorious-planet-1</strong>: <a href=\"https://wandb.ai/kristjan/NYT%20Multilabeling/runs/2gcsfx42\" target=\"_blank\">https://wandb.ai/kristjan/NYT%20Multilabeling/runs/2gcsfx42</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:2gcsfx42). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.27<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">good-firebrand-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/kristjan/NYT%20Multilabeling\" target=\"_blank\">https://wandb.ai/kristjan/NYT%20Multilabeling</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/kristjan/NYT%20Multilabeling/runs/lj1ew40v\" target=\"_blank\">https://wandb.ai/kristjan/NYT%20Multilabeling/runs/lj1ew40v</a><br/>\n",
              "                Run data is saved locally in <code>/content/drive/My Drive/Colab Notebooks/NLP/project/wandb/run-20210424_185438-lj1ew40v</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<wandb.wandb_torch.TorchGraph at 0x7f42ad8eb110>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOfidNXRdyty"
      },
      "source": [
        "def multi_label_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    confusion_vector = rounded_preds / y\n",
        "\n",
        "    true_positives = torch.sum(confusion_vector==1)\n",
        "    false_positives = torch.sum(torch.isinf(confusion_vector)) \n",
        "    false_negatives = torch.sum(confusion_vector==0)\n",
        "    true_negatives = torch.sum(torch.isnan(confusion_vector)) \n",
        "\n",
        "    accuracy = (true_positives + true_negatives) / (true_positives + false_positives + false_negatives + true_negatives)\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "    f_score = (2 * precision * recall) / (precision + recall)\n",
        "    \n",
        "    return accuracy, precision, recall, f_score"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWjj2IxpeBm0"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f_score = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch[0]).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch[1])\n",
        "        \n",
        "        acc, precision, recall, f_score = multi_label_accuracy(predictions, batch[1])\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        epoch_precision += precision.item()\n",
        "        epoch_recall += recall.item()\n",
        "        epoch_f_score += f_score.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), \\\n",
        "        epoch_precision / len(iterator), epoch_recall / len(iterator), \\\n",
        "        epoch_f_score / len(iterator)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_Rtg5SHeDcj"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f_score = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch[0]).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch[1])\n",
        "            \n",
        "            acc, precision, recall, f_score = multi_label_accuracy(predictions, batch[1])\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            epoch_precision += precision.item()\n",
        "            epoch_recall += recall.item()\n",
        "            epoch_f_score += f_score.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), \\\n",
        "        epoch_precision / len(iterator), epoch_recall / len(iterator), \\\n",
        "        epoch_f_score / len(iterator)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09eDs6MTeFXT"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ozh5_0NeHEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dbd7874-c5d2-4066-f1c4-a3f60e9fa601"
      },
      "source": [
        "%%time\n",
        "patience = 7\n",
        "epochs_of_no_improvement = 0\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "wandb.config.early_stopping_patience = patience\n",
        "wandb.config.training_samples=n_training_samples\n",
        "\n",
        "model_file_name = f'nyt_cnn_classifier_trained_with_{n_training_samples}_samples.pt'\n",
        "\n",
        "epoch = 0\n",
        "\n",
        "while True:\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc, train_precision, train_recall, train_f_score \\\n",
        "        = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc, valid_precision, valid_recall, valid_f_score \\\n",
        "        = evaluate(model, validation_loader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        print(f'New validation loss {valid_loss} is better than the best validation loss {best_valid_loss} so far.')\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), model_file_name)\n",
        "        epochs_of_no_improvement = 0\n",
        "    else: \n",
        "        epochs_of_no_improvement += 1\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | ' +\n",
        "          f'Train Precision: {train_precision*100:.2f}% | Train Recall: {train_recall*100:.2f}% | ' +\n",
        "          f'Train F1-score: {train_f_score*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | ' +\n",
        "          f'Val. Precision: {valid_precision*100:.2f}% | Val. Recall: {valid_recall*100:.2f}% | ' +\n",
        "          f'Val. F1-score: {valid_f_score*100:.2f}%')\n",
        "    \n",
        "    wandb.log({\"train_loss\": train_loss, \n",
        "                \"train_precision\": train_precision, \n",
        "                \"train_f_score\": train_f_score, \n",
        "                \"train_acc\": train_acc,\n",
        "                \"train_recall\": train_recall,\n",
        "               \"valid_loss\": valid_loss,\n",
        "               \"valid_acc\": valid_acc,\n",
        "               \"valid_precision\": valid_precision,\n",
        "               \"valid_recall\": valid_recall,\n",
        "               \"valid_f_score\": valid_f_score\n",
        "                })\n",
        "    # check if the training should be stopped and then stop the training\n",
        "    if epochs_of_no_improvement == patience : \n",
        "        print(f'Early stopping, on epoch: {epoch+1}.')\n",
        "        break\n",
        "    epoch += 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "New validation loss 0.024947953609069942 is better than the best validation loss inf so far.\n",
            "Epoch: 01 | Epoch Time: 2m 15s\n",
            "\tTrain Loss: 0.035 | Train Acc: 99.34% | Train Precision: nan% | Train Recall: 4.95% | Train F1-score: nan%\n",
            "\t Val. Loss: 0.025 |  Val. Acc: 99.47% | Val. Precision: 85.42% | Val. Recall: 9.40% | Val. F1-score: 16.84%\n",
            "New validation loss 0.021446241070273646 is better than the best validation loss 0.024947953609069942 so far.\n",
            "Epoch: 02 | Epoch Time: 2m 14s\n",
            "\tTrain Loss: 0.025 | Train Acc: 99.47% | Train Precision: 71.56% | Train Recall: 13.56% | Train F1-score: 22.56%\n",
            "\t Val. Loss: 0.021 |  Val. Acc: 99.50% | Val. Precision: 86.71% | Val. Recall: 15.51% | Val. F1-score: 26.14%\n",
            "New validation loss 0.019390962532416903 is better than the best validation loss 0.021446241070273646 so far.\n",
            "Epoch: 03 | Epoch Time: 2m 15s\n",
            "\tTrain Loss: 0.022 | Train Acc: 99.49% | Train Precision: 71.61% | Train Recall: 18.96% | Train F1-score: 29.75%\n",
            "\t Val. Loss: 0.019 |  Val. Acc: 99.52% | Val. Precision: 85.88% | Val. Recall: 19.45% | Val. F1-score: 31.56%\n",
            "New validation loss 0.018340739328855587 is better than the best validation loss 0.019390962532416903 so far.\n",
            "Epoch: 04 | Epoch Time: 2m 13s\n",
            "\tTrain Loss: 0.021 | Train Acc: 99.50% | Train Precision: 71.46% | Train Recall: 22.46% | Train F1-score: 33.94%\n",
            "\t Val. Loss: 0.018 |  Val. Acc: 99.53% | Val. Precision: 88.45% | Val. Recall: 20.66% | Val. F1-score: 33.34%\n",
            "New validation loss 0.01731984431842139 is better than the best validation loss 0.018340739328855587 so far.\n",
            "Epoch: 05 | Epoch Time: 2m 15s\n",
            "\tTrain Loss: 0.020 | Train Acc: 99.51% | Train Precision: 71.92% | Train Recall: 25.04% | Train F1-score: 36.91%\n",
            "\t Val. Loss: 0.017 |  Val. Acc: 99.54% | Val. Precision: 86.62% | Val. Recall: 24.27% | Val. F1-score: 37.74%\n",
            "New validation loss 0.017076751073795898 is better than the best validation loss 0.01731984431842139 so far.\n",
            "Epoch: 06 | Epoch Time: 2m 15s\n",
            "\tTrain Loss: 0.019 | Train Acc: 99.52% | Train Precision: 72.26% | Train Recall: 26.86% | Train F1-score: 38.93%\n",
            "\t Val. Loss: 0.017 |  Val. Acc: 99.55% | Val. Precision: 86.78% | Val. Recall: 24.86% | Val. F1-score: 38.48%\n",
            "New validation loss 0.016789673754760016 is better than the best validation loss 0.017076751073795898 so far.\n",
            "Epoch: 07 | Epoch Time: 2m 14s\n",
            "\tTrain Loss: 0.019 | Train Acc: 99.53% | Train Precision: 72.84% | Train Recall: 28.47% | Train F1-score: 40.71%\n",
            "\t Val. Loss: 0.017 |  Val. Acc: 99.55% | Val. Precision: 88.10% | Val. Recall: 24.87% | Val. F1-score: 38.60%\n",
            "New validation loss 0.016404241522094368 is better than the best validation loss 0.016789673754760016 so far.\n",
            "Epoch: 08 | Epoch Time: 2m 14s\n",
            "\tTrain Loss: 0.018 | Train Acc: 99.53% | Train Precision: 73.08% | Train Recall: 29.69% | Train F1-score: 41.98%\n",
            "\t Val. Loss: 0.016 |  Val. Acc: 99.56% | Val. Precision: 86.61% | Val. Recall: 27.19% | Val. F1-score: 41.20%\n",
            "New validation loss 0.016223169031213104 is better than the best validation loss 0.016404241522094368 so far.\n",
            "Epoch: 09 | Epoch Time: 2m 13s\n",
            "\tTrain Loss: 0.018 | Train Acc: 99.54% | Train Precision: 73.32% | Train Recall: 31.01% | Train F1-score: 43.34%\n",
            "\t Val. Loss: 0.016 |  Val. Acc: 99.56% | Val. Precision: 87.19% | Val. Recall: 27.59% | Val. F1-score: 41.73%\n",
            "Epoch: 10 | Epoch Time: 2m 14s\n",
            "\tTrain Loss: 0.018 | Train Acc: 99.54% | Train Precision: 73.75% | Train Recall: 31.86% | Train F1-score: 44.27%\n",
            "\t Val. Loss: 0.016 |  Val. Acc: 99.56% | Val. Precision: 87.75% | Val. Recall: 27.59% | Val. F1-score: 41.80%\n",
            "New validation loss 0.015824930539345854 is better than the best validation loss 0.016223169031213104 so far.\n",
            "Epoch: 11 | Epoch Time: 2m 14s\n",
            "\tTrain Loss: 0.017 | Train Acc: 99.55% | Train Precision: 73.83% | Train Recall: 32.80% | Train F1-score: 45.20%\n",
            "\t Val. Loss: 0.016 |  Val. Acc: 99.57% | Val. Precision: 84.90% | Val. Recall: 30.68% | Val. F1-score: 44.88%\n",
            "New validation loss 0.015703640224950573 is better than the best validation loss 0.015824930539345854 so far.\n",
            "Epoch: 12 | Epoch Time: 2m 15s\n",
            "\tTrain Loss: 0.017 | Train Acc: 99.55% | Train Precision: 74.56% | Train Recall: 33.71% | Train F1-score: 46.21%\n",
            "\t Val. Loss: 0.016 |  Val. Acc: 99.57% | Val. Precision: 86.52% | Val. Recall: 30.15% | Val. F1-score: 44.54%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0UINaoatRll"
      },
      "source": [
        "n_training_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSBf8rCBeIsK"
      },
      "source": [
        "model = CNN(pretrain, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_ID)\n",
        "model.load_state_dict(torch.load(model_file_name))\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM3lswCQebhC"
      },
      "source": [
        "start_time = time.time()\n",
        "test_loss, test_acc, test_precision, test_recall, test_f_score \\\n",
        "    = evaluate(model, test_loader, criterion)\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "print(f'Epoch: test | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | ' +\n",
        "      f'Test Precision: {test_precision*100:.2f}% | Test Recall: {test_recall*100:.2f}% | ' +\n",
        "      f'Test F1-score: {test_f_score*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFycOVvUedgE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}