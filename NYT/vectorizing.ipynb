{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Xi61aWUbEhcK"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from spacy.lang.en import English\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NUSrOjMIJlN4"
   },
   "outputs": [],
   "source": [
    "PAD = '<PAD>'\n",
    "PAD_ID = 0\n",
    "UNK = '<UNK>'\n",
    "UNK_ID = 1\n",
    "VOCAB_PREFIX = [PAD, UNK]\n",
    "\n",
    "VEC_PATH = Path('wiki-news-300d-1M.vec')\n",
    "DATA_PATH = Path('.')\n",
    "MAX_VOCAB = 25000\n",
    "\n",
    "batch_size = 64\n",
    "validation_split = .3\n",
    "shuffle_dataset = True\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oJx488cqJlPT"
   },
   "outputs": [],
   "source": [
    "class BaseVocab:\n",
    "    def __init__(self, data, lower=False):\n",
    "        self.data = data\n",
    "        self.lower = lower\n",
    "        self.build_vocab()\n",
    "        \n",
    "    def normalize_unit(self, unit):\n",
    "        if self.lower:\n",
    "            return unit.lower()\n",
    "        else:\n",
    "            return unit\n",
    "        \n",
    "    def unit2id(self, unit):\n",
    "        unit = self.normalize_unit(unit)\n",
    "        if unit in self._unit2id:\n",
    "            return self._unit2id[unit]\n",
    "        else:\n",
    "            return self._unit2id[UNK]\n",
    "    \n",
    "    def id2unit(self, id):\n",
    "        return self._id2unit[id]\n",
    "    \n",
    "    def map(self, units):\n",
    "        return [self.unit2id(unit) for unit in units]\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        NotImplementedError()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._unit2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QY-QgWo9JlTj"
   },
   "outputs": [],
   "source": [
    "class PretrainedWordVocab(BaseVocab):\n",
    "    def build_vocab(self):\n",
    "        self._id2unit = VOCAB_PREFIX + self.data\n",
    "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ar7b46jYJlXC"
   },
   "outputs": [],
   "source": [
    "class LabelVocab(BaseVocab):\n",
    "    def build_vocab(self):\n",
    "        self._id2unit = self.data\n",
    "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NBByipT4L4Kw"
   },
   "outputs": [],
   "source": [
    "class Pretrain:\n",
    "    def __init__(self, vec_filename, max_vocab=-1):\n",
    "        self._vec_filename = vec_filename\n",
    "        self._max_vocab = max_vocab\n",
    "        \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        if not hasattr(self, '_vocab'):\n",
    "            self._vocab, self._emb = self.read()\n",
    "        return self._vocab\n",
    "    \n",
    "    @property\n",
    "    def emb(self):\n",
    "        if not hasattr(self, '_emb'):\n",
    "            self._vocab, self._emb = self.read()\n",
    "        return self._emb\n",
    "        \n",
    "    def read(self):\n",
    "        if self._vec_filename is None:\n",
    "            raise Exception(\"Vector file is not provided.\")\n",
    "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
    "        \n",
    "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
    "        \n",
    "        if failed > 0: # recover failure\n",
    "            emb = emb[:-failed]\n",
    "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
    "            raise Exception(\"Loaded number of vectors does not match number of words.\")\n",
    "            \n",
    "        # Use a fixed vocab size\n",
    "        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n",
    "            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n",
    "            emb = emb[:self._max_vocab]\n",
    "                \n",
    "        vocab = PretrainedWordVocab(words, lower=True)\n",
    "        print(\"Done Reading\")\n",
    "        \n",
    "        return vocab, emb\n",
    "        \n",
    "    def read_from_file(self, filename, open_func=open):\n",
    "        \"\"\"\n",
    "        Open a vector file using the provided function and read from it.\n",
    "        \"\"\"\n",
    "        first = True\n",
    "        words = []\n",
    "        failed = 0\n",
    "        with open_func(filename, 'rb') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                try:\n",
    "                    line = line.decode()\n",
    "                except UnicodeDecodeError:\n",
    "                    failed += 1\n",
    "                    continue\n",
    "                if first:\n",
    "                    # the first line contains the number of word vectors and the dimensionality\n",
    "                    first = False\n",
    "                    line = line.strip().split(' ')\n",
    "                    rows, cols = [int(x) for x in line]\n",
    "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
    "                    continue\n",
    "\n",
    "                line = line.rstrip().split(' ')\n",
    "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
    "                words.append(' '.join(line[:-cols]))\n",
    "        return words, emb, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Kb7sjmY2nrNv"
   },
   "outputs": [],
   "source": [
    "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "5e0JyfjYL71w"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Vectorizer():\n",
    "    def __init__(self, pretrain, data, label_vocab=None, test=False):\n",
    "        self.pretrain_vocab = pretrain.vocab\n",
    "        self.data = data\n",
    "        self.test = test     \n",
    "        \n",
    "        if label_vocab is None:\n",
    "            labels = self.get_labels(data)\n",
    "            self.label_vocab = LabelVocab(labels)\n",
    "        else:\n",
    "            self.label_vocab = label_vocab\n",
    "        \n",
    "        \n",
    "    def get_labels(self, data):\n",
    "        labels = [a[3:] for a in data]\n",
    "        labels_flattened = []\n",
    "        for label in labels:\n",
    "            labels_flattened.extend(label)\n",
    "        self.labels = list(set(labels_flattened))\n",
    "        return self.labels\n",
    "\n",
    "    def vectorize(self, test_size):\n",
    "        articles = [a[2] for a in self.data]\n",
    "        labels = [a[3:] for a in self.data]\n",
    "        \n",
    "        nlp = English()\n",
    "        tokenizer = nlp.tokenizer\n",
    "        \n",
    "        vectorized_data = []\n",
    "        \n",
    "        x_train, x_val, y_train, y_val = train_test_split(articles, labels, test_size=test_size, random_state=0)\n",
    "        \n",
    "        if self.test:\n",
    "            articles, labels = x_val, y_val\n",
    "            print(f'Vectorizing {len(labels)} samples for test set')\n",
    "        else:\n",
    "            articles, labels = x_train, y_train\n",
    "            print(f'Vectorizing {len(labels)} samples for train set')\n",
    "\n",
    "               \n",
    "        for article, label_words in zip(articles, labels):\n",
    "            tokens = [t.text for t in tokenizer(article)]\n",
    "            text = torch.LongTensor(self.pretrain_vocab.map(tokens))\n",
    "\n",
    "            label_indices = torch.LongTensor(self.label_vocab.map(label_words))\n",
    "\n",
    "            n_labels = len(self.label_vocab)\n",
    "            src = torch.ones(n_labels)\n",
    "            label = torch.zeros(n_labels).scatter_(0, label_indices, src)\n",
    "            label = torch.FloatTensor(label)\n",
    "\n",
    "            vectorized_data.append((text, label))\n",
    "            \n",
    "        vectorized_data = sorted(vectorized_data, key=lambda x: len(x[0]), reverse=True)\n",
    "        \n",
    "        return vectorized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mY0-pAwpEn4B"
   },
   "outputs": [],
   "source": [
    "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3nsAvXhZjkZ",
    "outputId": "989b8fab-848e-4d89-bf7a-4e5f32e0c961"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check if we are running on a CPU or GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYDGJBkvk7gN",
    "outputId": "04f32f90-dcac-4214-f90b-1a66fbf543d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.97 s, sys: 5.8 s, total: 12.8 s\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "training_data_path = DATA_PATH / 'NYTcorpus_train.p'\n",
    "with open(training_data_path, mode='rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.Random(0).shuffle(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3IL9y07E1mj",
    "outputId": "68a7b282-316a-4160-f58d-1e0bf844a1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing 77910 samples for train set\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_size = 0.1\n",
    "\n",
    "for i, data_to_vectorize in enumerate(np.array_split(data, 15)):\n",
    "    start = time.time()\n",
    "\n",
    "    train_vectorizer = Vectorizer(pretrain, data_to_vectorize)\n",
    "\n",
    "    vectorized_data = train_vectorizer.vectorize(test_size=test_size)\n",
    "\n",
    "    torch.save(vectorized_data, f'all_vect/train/{i}_{len(vectorized_data)}.pt')\n",
    "\n",
    "    print(f\"{i} train vect time taken: {int(time.time() - start)} seconds\")\n",
    "    \n",
    "    start = time.time()\n",
    "    test_vectorizer = Vectorizer(pretrain, data_to_vectorize, label_vocab=train_vectorizer.label_vocab, test=True)\n",
    "\n",
    "    vectorized_data = test_vectorizer.vectorize(test_size=0.1)\n",
    "\n",
    "    torch.save(vectorized_data, f'all_vect/test/{i}_{len(vectorized_data)}.pt')\n",
    "    print(f\"{i} test vect time taken: {int(time.time() - start)} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NYT-CNN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
