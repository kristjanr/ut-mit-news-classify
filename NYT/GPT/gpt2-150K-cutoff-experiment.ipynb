{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions (run first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import requests\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTEmbeddedDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=['suspensions, dismissals and resignations',\n",
       "                             'education and schools',\n",
       "                             'colleges and universities', 'blacks',\n",
       "                             'population', 'economic conditions and trends',\n",
       "                             'labor',\n",
       "                             'office buildings and commercial properties',\n",
       "                             'architecture', 'medicine and health',\n",
       "                             'awards, decorations and honors',\n",
       "                             'diseases and conditions', 'research', 'cancer',\n",
       "                             'basketball', 'design', 'interior design',\n",
       "                             'real estate', 'trades (sports)',\n",
       "                             'demonstrations and riots', 'dancing',\n",
       "                             'hockey, ice', 'games', 'playoff games',\n",
       "                             'baseball', 'travel and vacations', 'finances',\n",
       "                             'books and literature',\n",
       "                             'united states politics and government',\n",
       "                             'politics and government', ...])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def loadcsv(filename):\n",
    "    with open(filename, newline='', encoding='utf-8') as f:\n",
    "        return list(csv.reader(f))\n",
    "\n",
    "def load_label_map(out2id_path, id2label_path):\n",
    "    \n",
    "    out2id = loadcsv(out2id_path)\n",
    "    out2id = {int(row[0]): row[1] for row in out2id}\n",
    "\n",
    "    id2label_raw = loadcsv(id2label_path)\n",
    "    id2label = {}\n",
    "\n",
    "    for row in id2label_raw:\n",
    "        if row == []:\n",
    "            continue\n",
    "        id2label[row[1]] = row[2]\n",
    "\n",
    "    out2label = [id2label[out2id[out]] for out in sorted(out2id.keys())]\n",
    "    \n",
    "    return out2label\n",
    "\n",
    "out2label = load_label_map('../data/labels_dict_gpt.csv', '../data/nyt-theme-tags.csv')\n",
    "mlb = MultiLabelBinarizer(classes=out2label)\n",
    "mlb.fit(out2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def validation_split(dataset, validation_subset, seed=42):\n",
    "\n",
    "    if validation_subset > 0:\n",
    "        n_total_samples = len(dataset)\n",
    "        n_train_samples = math.floor(n_total_samples * (1-validation_subset))\n",
    "        n_valid_samples = n_total_samples - n_train_samples\n",
    "\n",
    "        train_subset, valid_subset = random_split(\n",
    "            dataset,\n",
    "            [n_train_samples, n_valid_samples],\n",
    "            generator=torch.Generator().manual_seed(seed)\n",
    "        )  # reproducible results\n",
    "\n",
    "    else:\n",
    "        train_subset = dataset\n",
    "        valid_subset = None\n",
    "\n",
    "    return train_subset, valid_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f_score = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "\n",
    "        article_embeddings, labels, idx  = batch\n",
    "        article_embeddings = article_embeddings.to(device)\n",
    "        labels = labels.type(torch.float).to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(article_embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate metrics\n",
    "        preds = model.act(outputs) > 0.5\n",
    "\n",
    "        acc, precision, recall, f1 = multi_label_scores(labels.detach().cpu(), preds.detach().cpu())\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_precision += precision.item()\n",
    "        epoch_recall += recall.item()\n",
    "        epoch_f_score += f1.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), \\\n",
    "        epoch_precision / len(iterator), epoch_recall / len(iterator), \\\n",
    "        epoch_f_score / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f_score = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            article_embeddings, labels, idx  = batch\n",
    "            article_embeddings = article_embeddings.to(device)\n",
    "            labels = labels.type(torch.float).to(device)\n",
    "\n",
    "            outputs = model(article_embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # calculate metrics\n",
    "            preds = model.act(outputs) > 0.5\n",
    "\n",
    "            acc, precision, recall, f1 = multi_label_scores(labels.detach().cpu(), preds.detach().cpu())\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_precision += precision.item()\n",
    "            epoch_recall += recall.item()\n",
    "            epoch_f_score += f1.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), \\\n",
    "        epoch_precision / len(iterator), epoch_recall / len(iterator), \\\n",
    "        epoch_f_score / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from mitnewsclassify2.gpt_model import GPTModel as GPTHead2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def multi_label_scores(correct_labels, predicted_labels):\n",
    "\n",
    "    accuracy = accuracy_score(correct_labels, predicted_labels)\n",
    "    precision = precision_score(correct_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(correct_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "    f_1_score = f1_score(correct_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "    \n",
    "    return accuracy, precision, recall, f_1_score\n",
    "\n",
    "def gettags(head_model, features, eval=False):\n",
    "    head_model.eval()\n",
    "    features = features.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = head_model(features)\n",
    "        multi_label_sigmoids = head_model.act(logits)\n",
    "\n",
    "    preds = multi_label_sigmoids > 0.5\n",
    "    preds = preds.detach().cpu()\n",
    "\n",
    "    return mlb.inverse_transform(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mut-mit-news-classify\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_complete torch.Size([150000, 768])\n",
      "y_train_complete torch.Size([150000, 538])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">curious-serenity-33</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ut-mit-news-classify/NYT%20Multilabeling\" target=\"_blank\">https://wandb.ai/ut-mit-news-classify/NYT%20Multilabeling</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ut-mit-news-classify/NYT%20Multilabeling/runs/1y7wfxbv\" target=\"_blank\">https://wandb.ai/ut-mit-news-classify/NYT%20Multilabeling/runs/1y7wfxbv</a><br/>\n",
       "                Run data is saved locally in <code>/gpfs/space/home/mykyta/nlp/ut-mit-news-classify/NYT/GPT/wandb/run-20210521_205452-1y7wfxbv</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "New validation loss 0.5034549271656295 is better than the best validation loss inf so far.\n",
      "Epoch: 01 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.608 | Train Acc: 3.20% | Train Precision: 7.38% | Train Recall: 19.70% | Train F1-score: 8.26%\n",
      "\t Val. Loss: 0.503 |  Val. Acc: 7.48% | Val. Precision: 12.42% | Val. Recall: 18.04% | Val. F1-score: 13.45%\n",
      "New validation loss 0.3402182116346844 is better than the best validation loss 0.5034549271656295 so far.\n",
      "Epoch: 02 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.410 | Train Acc: 8.53% | Train Precision: 15.68% | Train Recall: 19.49% | Train F1-score: 16.50%\n",
      "\t Val. Loss: 0.340 |  Val. Acc: 9.68% | Val. Precision: 17.37% | Val. Recall: 19.07% | Val. F1-score: 17.51%\n",
      "New validation loss 0.2437568887815637 is better than the best validation loss 0.3402182116346844 so far.\n",
      "Epoch: 03 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.288 | Train Acc: 11.23% | Train Precision: 21.04% | Train Recall: 19.48% | Train F1-score: 19.54%\n",
      "\t Val. Loss: 0.244 |  Val. Acc: 12.71% | Val. Precision: 23.06% | Val. Recall: 20.96% | Val. F1-score: 20.84%\n",
      "New validation loss 0.18181691432403305 is better than the best validation loss 0.2437568887815637 so far.\n",
      "Epoch: 04 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.211 | Train Acc: 15.11% | Train Precision: 27.32% | Train Recall: 22.03% | Train F1-score: 23.46%\n",
      "\t Val. Loss: 0.182 |  Val. Acc: 17.17% | Val. Precision: 28.89% | Val. Recall: 23.33% | Val. F1-score: 24.36%\n",
      "New validation loss 0.1388605633529566 is better than the best validation loss 0.18181691432403305 so far.\n",
      "Epoch: 05 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.159 | Train Acc: 18.76% | Train Precision: 32.98% | Train Recall: 24.58% | Train F1-score: 27.05%\n",
      "\t Val. Loss: 0.139 |  Val. Acc: 19.13% | Val. Precision: 32.76% | Val. Recall: 25.38% | Val. F1-score: 26.87%\n",
      "New validation loss 0.10849694819268534 is better than the best validation loss 0.1388605633529566 so far.\n",
      "Epoch: 06 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.123 | Train Acc: 20.54% | Train Precision: 37.83% | Train Recall: 26.36% | Train F1-score: 29.65%\n",
      "\t Val. Loss: 0.108 |  Val. Acc: 19.92% | Val. Precision: 37.19% | Val. Recall: 25.95% | Val. F1-score: 28.42%\n",
      "New validation loss 0.08642650281978866 is better than the best validation loss 0.10849694819268534 so far.\n",
      "Epoch: 07 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.096 | Train Acc: 21.86% | Train Precision: 42.52% | Train Recall: 28.13% | Train F1-score: 32.20%\n",
      "\t Val. Loss: 0.086 |  Val. Acc: 21.16% | Val. Precision: 39.78% | Val. Recall: 26.89% | Val. F1-score: 30.06%\n",
      "New validation loss 0.0702446442791971 is better than the best validation loss 0.08642650281978866 so far.\n",
      "Epoch: 08 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.077 | Train Acc: 22.90% | Train Precision: 45.66% | Train Recall: 29.59% | Train F1-score: 34.15%\n",
      "\t Val. Loss: 0.070 |  Val. Acc: 21.51% | Val. Precision: 41.05% | Val. Recall: 27.74% | Val. F1-score: 31.05%\n",
      "New validation loss 0.05787142776584221 is better than the best validation loss 0.0702446442791971 so far.\n",
      "Epoch: 09 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.063 | Train Acc: 23.66% | Train Precision: 48.11% | Train Recall: 30.92% | Train F1-score: 35.83%\n",
      "\t Val. Loss: 0.058 |  Val. Acc: 22.70% | Val. Precision: 42.42% | Val. Recall: 28.16% | Val. F1-score: 31.84%\n",
      "New validation loss 0.048885112533629954 is better than the best validation loss 0.05787142776584221 so far.\n",
      "Epoch: 10 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.052 | Train Acc: 24.27% | Train Precision: 50.14% | Train Recall: 32.19% | Train F1-score: 37.36%\n",
      "\t Val. Loss: 0.049 |  Val. Acc: 22.47% | Val. Precision: 43.49% | Val. Recall: 28.53% | Val. F1-score: 32.07%\n",
      "New validation loss 0.041823093353186624 is better than the best validation loss 0.048885112533629954 so far.\n",
      "Epoch: 11 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.044 | Train Acc: 24.82% | Train Precision: 52.06% | Train Recall: 33.57% | Train F1-score: 38.93%\n",
      "\t Val. Loss: 0.042 |  Val. Acc: 22.33% | Val. Precision: 44.44% | Val. Recall: 27.37% | Val. F1-score: 31.73%\n",
      "New validation loss 0.03655085264373634 is better than the best validation loss 0.041823093353186624 so far.\n",
      "Epoch: 12 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.037 | Train Acc: 25.37% | Train Precision: 53.82% | Train Recall: 34.87% | Train F1-score: 40.40%\n",
      "\t Val. Loss: 0.037 |  Val. Acc: 22.11% | Val. Precision: 44.54% | Val. Recall: 27.50% | Val. F1-score: 31.66%\n",
      "New validation loss 0.03213529146702613 is better than the best validation loss 0.03655085264373634 so far.\n",
      "Epoch: 13 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.032 | Train Acc: 25.90% | Train Precision: 55.42% | Train Recall: 36.23% | Train F1-score: 41.86%\n",
      "\t Val. Loss: 0.032 |  Val. Acc: 22.93% | Val. Precision: 46.54% | Val. Recall: 28.67% | Val. F1-score: 33.18%\n",
      "New validation loss 0.02894769470065327 is better than the best validation loss 0.03213529146702613 so far.\n",
      "Epoch: 14 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.028 | Train Acc: 26.45% | Train Precision: 56.82% | Train Recall: 37.55% | Train F1-score: 43.25%\n",
      "\t Val. Loss: 0.029 |  Val. Acc: 22.98% | Val. Precision: 47.41% | Val. Recall: 28.44% | Val. F1-score: 33.42%\n",
      "New validation loss 0.026622197543412954 is better than the best validation loss 0.02894769470065327 so far.\n",
      "Epoch: 15 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.025 | Train Acc: 26.97% | Train Precision: 58.11% | Train Recall: 38.81% | Train F1-score: 44.54%\n",
      "\t Val. Loss: 0.027 |  Val. Acc: 22.78% | Val. Precision: 46.30% | Val. Recall: 29.87% | Val. F1-score: 34.06%\n",
      "New validation loss 0.024695817728416395 is better than the best validation loss 0.026622197543412954 so far.\n",
      "Epoch: 16 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.022 | Train Acc: 27.47% | Train Precision: 59.40% | Train Recall: 40.08% | Train F1-score: 45.87%\n",
      "\t Val. Loss: 0.025 |  Val. Acc: 23.07% | Val. Precision: 47.42% | Val. Recall: 28.88% | Val. F1-score: 33.84%\n",
      "New validation loss 0.023208467289805412 is better than the best validation loss 0.024695817728416395 so far.\n",
      "Epoch: 17 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.020 | Train Acc: 27.97% | Train Precision: 60.53% | Train Recall: 41.29% | Train F1-score: 47.10%\n",
      "\t Val. Loss: 0.023 |  Val. Acc: 23.51% | Val. Precision: 48.59% | Val. Recall: 30.37% | Val. F1-score: 35.35%\n",
      "New validation loss 0.022075509178941532 is better than the best validation loss 0.023208467289805412 so far.\n",
      "Epoch: 18 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.019 | Train Acc: 28.47% | Train Precision: 61.70% | Train Recall: 42.56% | Train F1-score: 48.39%\n",
      "\t Val. Loss: 0.022 |  Val. Acc: 23.31% | Val. Precision: 50.09% | Val. Recall: 31.77% | Val. F1-score: 36.78%\n",
      "New validation loss 0.021483261864316667 is better than the best validation loss 0.022075509178941532 so far.\n",
      "Epoch: 19 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.017 | Train Acc: 28.98% | Train Precision: 62.76% | Train Recall: 43.78% | Train F1-score: 49.60%\n",
      "\t Val. Loss: 0.021 |  Val. Acc: 23.32% | Val. Precision: 49.11% | Val. Recall: 32.31% | Val. F1-score: 36.73%\n",
      "New validation loss 0.020777545837780177 is better than the best validation loss 0.021483261864316667 so far.\n",
      "Epoch: 20 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.016 | Train Acc: 29.44% | Train Precision: 63.72% | Train Recall: 44.93% | Train F1-score: 50.71%\n",
      "\t Val. Loss: 0.021 |  Val. Acc: 23.65% | Val. Precision: 49.83% | Val. Recall: 30.53% | Val. F1-score: 35.90%\n",
      "New validation loss 0.020604972711811633 is better than the best validation loss 0.020777545837780177 so far.\n",
      "Epoch: 21 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.015 | Train Acc: 29.91% | Train Precision: 64.80% | Train Recall: 46.17% | Train F1-score: 51.94%\n",
      "\t Val. Loss: 0.021 |  Val. Acc: 23.90% | Val. Precision: 50.56% | Val. Recall: 32.57% | Val. F1-score: 37.46%\n",
      "New validation loss 0.020336157500238743 is better than the best validation loss 0.020604972711811633 so far.\n",
      "Epoch: 22 | Epoch Time: 0m 43s\n",
      "\tTrain Loss: 0.014 | Train Acc: 30.32% | Train Precision: 65.74% | Train Recall: 47.28% | Train F1-score: 53.03%\n",
      "\t Val. Loss: 0.020 |  Val. Acc: 23.64% | Val. Precision: 50.27% | Val. Recall: 31.63% | Val. F1-score: 36.85%\n",
      "Epoch: 23 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.014 | Train Acc: 30.82% | Train Precision: 66.63% | Train Recall: 48.39% | Train F1-score: 54.08%\n",
      "\t Val. Loss: 0.020 |  Val. Acc: 23.61% | Val. Precision: 50.06% | Val. Recall: 33.39% | Val. F1-score: 37.98%\n",
      "Epoch: 24 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.013 | Train Acc: 31.18% | Train Precision: 67.54% | Train Recall: 49.35% | Train F1-score: 55.05%\n",
      "\t Val. Loss: 0.020 |  Val. Acc: 23.55% | Val. Precision: 50.56% | Val. Recall: 32.87% | Val. F1-score: 37.67%\n",
      "Epoch: 25 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.013 | Train Acc: 31.66% | Train Precision: 68.34% | Train Recall: 50.42% | Train F1-score: 56.04%\n",
      "\t Val. Loss: 0.021 |  Val. Acc: 22.55% | Val. Precision: 49.91% | Val. Recall: 29.24% | Val. F1-score: 34.61%\n",
      "Epoch: 26 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.012 | Train Acc: 32.06% | Train Precision: 68.98% | Train Recall: 51.35% | Train F1-score: 56.92%\n",
      "\t Val. Loss: 0.021 |  Val. Acc: 23.30% | Val. Precision: 50.51% | Val. Recall: 31.16% | Val. F1-score: 36.43%\n",
      "Epoch: 27 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.012 | Train Acc: 32.60% | Train Precision: 69.84% | Train Recall: 52.31% | Train F1-score: 57.88%\n",
      "\t Val. Loss: 0.021 |  Val. Acc: 23.25% | Val. Precision: 50.48% | Val. Recall: 32.41% | Val. F1-score: 37.32%\n",
      "Epoch: 28 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.012 | Train Acc: 32.93% | Train Precision: 70.64% | Train Recall: 53.23% | Train F1-score: 58.76%\n",
      "\t Val. Loss: 0.021 |  Val. Acc: 23.39% | Val. Precision: 50.10% | Val. Recall: 30.07% | Val. F1-score: 35.46%\n",
      "Epoch: 29 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.011 | Train Acc: 33.36% | Train Precision: 71.24% | Train Recall: 54.10% | Train F1-score: 59.56%\n",
      "\t Val. Loss: 0.021 |  Val. Acc: 23.38% | Val. Precision: 50.88% | Val. Recall: 32.81% | Val. F1-score: 37.89%\n",
      "Epoch: 30 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.011 | Train Acc: 33.89% | Train Precision: 71.95% | Train Recall: 55.02% | Train F1-score: 60.42%\n",
      "\t Val. Loss: 0.021 |  Val. Acc: 23.64% | Val. Precision: 50.99% | Val. Recall: 31.98% | Val. F1-score: 37.16%\n",
      "Epoch: 31 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.011 | Train Acc: 34.14% | Train Precision: 72.50% | Train Recall: 55.71% | Train F1-score: 61.10%\n",
      "\t Val. Loss: 0.021 |  Val. Acc: 23.45% | Val. Precision: 50.82% | Val. Recall: 32.31% | Val. F1-score: 37.39%\n",
      "Epoch: 32 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.010 | Train Acc: 34.54% | Train Precision: 73.09% | Train Recall: 56.42% | Train F1-score: 61.76%\n",
      "\t Val. Loss: 0.021 |  Val. Acc: 22.93% | Val. Precision: 50.13% | Val. Recall: 32.18% | Val. F1-score: 37.11%\n",
      "Early stopping, on epoch: 32.\n",
      "Testing...\n",
      "Epoch: test | Epoch Time: 0m 4s | Dataset: cutoff\n",
      "\tTest Loss: 0.026 | Test Acc: 10.26% | Test Precision: 32.86% | Test Recall: 18.64% | Test F1-score: 22.41%\n",
      "Epoch: test | Epoch Time: 0m 4s | Dataset: complete\n",
      "\tTest Loss: 0.020 | Test Acc: 23.53% | Test Precision: 46.80% | Test Recall: 31.37% | Test F1-score: 35.68%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10358<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71063ef1b924c30acbd5b0e20b4c7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/gpfs/space/home/mykyta/nlp/ut-mit-news-classify/NYT/GPT/wandb/run-20210521_205452-1y7wfxbv/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/gpfs/space/home/mykyta/nlp/ut-mit-news-classify/NYT/GPT/wandb/run-20210521_205452-1y7wfxbv/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss</td><td>0.0105</td></tr><tr><td>train_precision</td><td>0.73088</td></tr><tr><td>train_f_score</td><td>0.61764</td></tr><tr><td>train_acc</td><td>0.34537</td></tr><tr><td>train_recall</td><td>0.56424</td></tr><tr><td>valid_loss</td><td>0.02128</td></tr><tr><td>valid_acc</td><td>0.22927</td></tr><tr><td>valid_precision</td><td>0.50132</td></tr><tr><td>valid_recall</td><td>0.32183</td></tr><tr><td>valid_f_score</td><td>0.37105</td></tr><tr><td>epoch</td><td>32</td></tr><tr><td>_runtime</td><td>1348</td></tr><tr><td>_timestamp</td><td>1621621040</td></tr><tr><td>_step</td><td>31</td></tr><tr><td>test_cutoff_acc</td><td>0.10258</td></tr><tr><td>test_cutoff_precision</td><td>0.32856</td></tr><tr><td>test_cutoff_recall</td><td>0.18644</td></tr><tr><td>test_cutoff_f_score</td><td>0.22411</td></tr><tr><td>test_complete_acc</td><td>0.23535</td></tr><tr><td>test_complete_precision</td><td>0.46799</td></tr><tr><td>test_complete_recall</td><td>0.31365</td></tr><tr><td>test_complete_f_score</td><td>0.35678</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_precision</td><td>▁▂▂▃▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>train_f_score</td><td>▁▂▂▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_acc</td><td>▁▂▃▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_recall</td><td>▁▁▁▁▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>valid_loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_acc</td><td>▁▂▃▅▆▆▇▇▇▇▇▇████████████▇███████</td></tr><tr><td>valid_precision</td><td>▁▂▃▄▅▅▆▆▆▇▇▇▇▇▇▇████████████████</td></tr><tr><td>valid_recall</td><td>▁▁▂▃▄▅▅▅▆▆▅▅▆▆▆▆▇▇█▇█▇██▆▇█▆█▇█▇</td></tr><tr><td>valid_f_score</td><td>▁▂▃▄▅▅▆▆▆▆▆▆▇▇▇▇▇██▇████▇██▇████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">curious-serenity-33</strong>: <a href=\"https://wandb.ai/ut-mit-news-classify/NYT%20Multilabeling/runs/1y7wfxbv\" target=\"_blank\">https://wandb.ai/ut-mit-news-classify/NYT%20Multilabeling/runs/1y7wfxbv</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 14s, sys: 1min 48s, total: 23min 3s\n",
      "Wall time: 22min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# for ARTICLE_TYPE in ['cutoff', 'complete']:\n",
    "for ARTICLE_TYPE in ['complete']:\n",
    "\n",
    "    train_dataset = torch.load(f'vectorized/train_150k_{ARTICLE_TYPE}.pt')\n",
    "\n",
    "    print(f'X_train_{ARTICLE_TYPE}', train_dataset.X.shape)\n",
    "    print(f'y_train_{ARTICLE_TYPE}', train_dataset.y.shape)\n",
    "\n",
    "    # splitting train/validation\n",
    "    batch_size = 256\n",
    "    seed = 42\n",
    "\n",
    "    train_subset, valid_subset = validation_split(train_dataset, 0.1, seed)\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_subset, batch_size=batch_size)\n",
    "\n",
    "    n_training_samples = train_dataset.X.shape[0]\n",
    "\n",
    "    # hyperparams\n",
    "    max_epochs = 1000\n",
    "\n",
    "    patience = 10\n",
    "\n",
    "    # model\n",
    "    model_path = f'models/{ARTICLE_TYPE}.pt'\n",
    "    model = GPTHead2(768, 538).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                        lr = 1e-3, # default is 5e-5, our notebook had 2e-5\n",
    "                      )\n",
    "\n",
    "    wandb.init(\n",
    "        entity='ut-mit-news-classify',\n",
    "        project=\"NYT Multilabeling\",\n",
    "        tags=[ARTICLE_TYPE, 'cutoff-experiment'],\n",
    "    )\n",
    "\n",
    "    # training\n",
    "    epochs_of_no_improvement = 0\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    wandb.config.early_stopping_patience = patience\n",
    "    wandb.config.training_samples=n_training_samples\n",
    "    wandb.config.model_path = model_path\n",
    "\n",
    "    wandb.save(model_path)  # this will make wandb upload the model after call to `wandb.finish()`\n",
    "    \n",
    "    print('Starting training...')\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc, train_precision, train_recall, train_f_score \\\n",
    "            = train(model, train_loader, optimizer, criterion)\n",
    "        valid_loss, valid_acc, valid_precision, valid_recall, valid_f_score \\\n",
    "            = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            print(f'New validation loss {valid_loss} is better than the best validation loss {best_valid_loss} so far.')\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model, model_path)\n",
    "            epochs_of_no_improvement = 0\n",
    "        else: \n",
    "            epochs_of_no_improvement += 1\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | ' +\n",
    "              f'Train Precision: {train_precision*100:.2f}% | Train Recall: {train_recall*100:.2f}% | ' +\n",
    "              f'Train F1-score: {train_f_score*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | ' +\n",
    "              f'Val. Precision: {valid_precision*100:.2f}% | Val. Recall: {valid_recall*100:.2f}% | ' +\n",
    "              f'Val. F1-score: {valid_f_score*100:.2f}%')\n",
    "\n",
    "        wandb.log({\"train_loss\": train_loss, \n",
    "                    \"train_precision\": train_precision, \n",
    "                    \"train_f_score\": train_f_score, \n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"train_recall\": train_recall,\n",
    "                   \"valid_loss\": valid_loss,\n",
    "                   \"valid_acc\": valid_acc,\n",
    "                   \"valid_precision\": valid_precision,\n",
    "                   \"valid_recall\": valid_recall,\n",
    "                   \"valid_f_score\": valid_f_score,\n",
    "                   \"epoch\": epoch+1,\n",
    "                    })\n",
    "        # check if the training should be stopped and then stop the training\n",
    "        if epochs_of_no_improvement == patience : \n",
    "            print(f'Early stopping, on epoch: {epoch+1}.')\n",
    "            break\n",
    "\n",
    "    del train_dataset\n",
    "    del train_subset\n",
    "    del valid_subset\n",
    "    del train_loader\n",
    "    del valid_loader\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    ####### TESTING #######\n",
    "    \n",
    "    print('Testing...')\n",
    "    \n",
    "    \n",
    "    model = torch.load(model_path)\n",
    "\n",
    "    test_dataset_names = [('cutoff', 'vectorized/test_150k_cutoff.pt'), ('complete', 'vectorized/test_150k_complete.pt')]\n",
    "\n",
    "    for article_type, dataset_path in test_dataset_names:\n",
    "        test_dataset = torch.load(dataset_path)\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "        start_time = time.time()\n",
    "        test_loss, test_acc, test_precision, test_recall, test_f_score \\\n",
    "            = evaluate(model, test_loader, criterion)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        wandb.run.summary[f'test_{article_type}_acc'] = test_acc\n",
    "        wandb.run.summary[f'test_{article_type}_precision'] = test_precision\n",
    "        wandb.run.summary[f'test_{article_type}_recall'] = test_recall\n",
    "        wandb.run.summary[f'test_{article_type}_f_score'] = test_f_score\n",
    "\n",
    "        print(f'Epoch: test | Epoch Time: {epoch_mins}m {epoch_secs}s | Dataset: {article_type}')\n",
    "        print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | ' +\n",
    "              f'Test Precision: {test_precision*100:.2f}% | Test Recall: {test_recall*100:.2f}% | ' +\n",
    "              f'Test F1-score: {test_f_score*100:.2f}%')\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: test | Epoch Time: 0m 4s | Dataset: cutoff\n",
      "\tTest Loss: 0.023 | Test Acc: 17.66% | Test Precision: 39.19% | Test Recall: 27.00% | Test F1-score: 30.26%\n",
      "Epoch: test | Epoch Time: 0m 4s | Dataset: complete\n",
      "\tTest Loss: 0.021 | Test Acc: 20.41% | Test Precision: 42.71% | Test Recall: 31.10% | Test F1-score: 34.10%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25440<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589277232cee46648eb7148e44c3bbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/gpfs/space/home/mykyta/nlp/ut-mit-news-classify/NYT/GPT/wandb/run-20210521_200449-7z2a9l2e/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/gpfs/space/home/mykyta/nlp/ut-mit-news-classify/NYT/GPT/wandb/run-20210521_200449-7z2a9l2e/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss</td><td>0.01194</td></tr><tr><td>train_precision</td><td>0.68204</td></tr><tr><td>train_f_score</td><td>0.55809</td></tr><tr><td>train_acc</td><td>0.29066</td></tr><tr><td>train_recall</td><td>0.50251</td></tr><tr><td>valid_loss</td><td>0.02432</td></tr><tr><td>valid_acc</td><td>0.1849</td></tr><tr><td>valid_precision</td><td>0.40368</td></tr><tr><td>valid_recall</td><td>0.2731</td></tr><tr><td>valid_f_score</td><td>0.30717</td></tr><tr><td>epoch</td><td>34</td></tr><tr><td>_runtime</td><td>1378</td></tr><tr><td>_timestamp</td><td>1621618070</td></tr><tr><td>_step</td><td>33</td></tr><tr><td>test_cutoff_acc</td><td>0.17662</td></tr><tr><td>test_cutoff_precision</td><td>0.39193</td></tr><tr><td>test_cutoff_recall</td><td>0.26996</td></tr><tr><td>test_cutoff_f_score</td><td>0.30261</td></tr><tr><td>test_complete_acc</td><td>0.20412</td></tr><tr><td>test_complete_precision</td><td>0.42713</td></tr><tr><td>test_complete_recall</td><td>0.31097</td></tr><tr><td>test_complete_f_score</td><td>0.34098</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_precision</td><td>▁▁▂▂▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>train_f_score</td><td>▁▁▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██████</td></tr><tr><td>train_acc</td><td>▁▂▂▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_recall</td><td>▃▁▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>valid_loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_acc</td><td>▁▂▂▃▄▅▆▆▇▇▇▇▇█████████████████████</td></tr><tr><td>valid_precision</td><td>▁▂▃▄▄▅▆▆▇▇▇▇█▇████████████████████</td></tr><tr><td>valid_recall</td><td>▁▂▃▄▄▅▅▅▆▆▇▆▇▇▇▇▇▇███████▇████▇▇██</td></tr><tr><td>valid_f_score</td><td>▁▂▃▄▅▅▅▆▆▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">flowing-oath-32</strong>: <a href=\"https://wandb.ai/ut-mit-news-classify/NYT%20Multilabeling/runs/7z2a9l2e\" target=\"_blank\">https://wandb.ai/ut-mit-news-classify/NYT%20Multilabeling/runs/7z2a9l2e</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torch.load(model_path)\n",
    "\n",
    "test_dataset_names = [('cutoff', 'vectorized/test_150k_cutoff.pt'), ('complete', 'vectorized/test_150k_complete.pt')]\n",
    "\n",
    "for article_type, dataset_path in test_dataset_names:\n",
    "    test_dataset = torch.load(dataset_path)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_loss, test_acc, test_precision, test_recall, test_f_score \\\n",
    "        = evaluate(model, test_loader, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    wandb.run.summary[f'test_{article_type}_acc'] = test_acc\n",
    "    wandb.run.summary[f'test_{article_type}_precision'] = test_precision\n",
    "    wandb.run.summary[f'test_{article_type}_recall'] = test_recall\n",
    "    wandb.run.summary[f'test_{article_type}_f_score'] = test_f_score\n",
    "\n",
    "    print(f'Epoch: test | Epoch Time: {epoch_mins}m {epoch_secs}s | Dataset: {article_type}')\n",
    "    print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | ' +\n",
    "          f'Test Precision: {test_precision*100:.2f}% | Test Recall: {test_recall*100:.2f}% | ' +\n",
    "          f'Test F1-score: {test_f_score*100:.2f}%')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: test | Epoch Time: 0m 1s\n",
      "\tTest Loss: 0.018 | Test Acc: 23.51% | Test Precision: 45.93% | Test Recall: 31.37% | Test F1-score: 35.50%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted [('politics and government',)]\n",
      "gold: [('armament, defense and military forces',)]\n"
     ]
    }
   ],
   "source": [
    "idx = 15\n",
    "print('predicted', gettags(model, train_dataset.X[idx]))\n",
    "print('gold:', mlb.inverse_transform(train_dataset.y[idx].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
