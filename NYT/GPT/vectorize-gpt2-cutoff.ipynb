{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports seem good!\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = \"/gpfs/space/home/mykyta/nlp/ut-mit-news-classify/NYT/\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from utils import print_f\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print_f('All imports seem good!')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print_f('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NYT dataset...\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "from utils import GPTVectorizedDataset\n",
    "\n",
    "MODEL = 'gpt2'\n",
    "batch_size = 8\n",
    "chunk_size = 200_000\n",
    "\n",
    "tokenized_train_path = f'tokenized/train_150k_min500_complete.pt'\n",
    "tokenized_test_path = f'tokenized/test_150k_min500_complete.pt'\n",
    "\n",
    "os.makedirs('vectorized', exist_ok=True)\n",
    "\n",
    "vectorized_train_path = f'vectorized/train_150k_min500_complete.pt'\n",
    "vectorized_test_path = f'vectorized/test_150k_min500_complete.pt'\n",
    "\n",
    "print_f('Loading NYT dataset...')\n",
    "\n",
    "train_dataset = torch.load(tokenized_train_path)\n",
    "test_dataset = torch.load(tokenized_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chunks 1\n",
      "chunks []\n",
      "Vectorizing dataset for  vectorized/train_150k_min500_complete.pt\n",
      "Starting at chunk id 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56c081fbcb54a4ab1d8d08528693a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f5606d7a63cb>:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving chunk: vectorized/train_150k_min500_complete.pt\n",
      "total chunks 1\n",
      "chunks []\n",
      "Vectorizing dataset for  vectorized/test_150k_min500_complete.pt\n",
      "Starting at chunk id 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ce3b6c673d45188204772bde6a94fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving chunk: vectorized/test_150k_min500_complete.pt\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# start actual vectorization with GPT2\n",
    "runs = [(train_dataset, vectorized_train_path), (test_dataset, vectorized_test_path)]\n",
    "\n",
    "print_f('Loading model...')\n",
    "model = GPT2Model.from_pretrained(MODEL)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(test_dataset.tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Load model to defined device.\n",
    "model.to(device)\n",
    "\n",
    "for dataset, output_path in runs:\n",
    "\n",
    "    total_chunks = len(dataset) // chunk_size + 1\n",
    "    print_f('total chunks', total_chunks)\n",
    "\n",
    "    # skip already embedded articles\n",
    "    skip_n_articles = 0\n",
    "    chunk_paths = sorted([chunk_path for chunk_path in os.listdir('.') if f'{output_path}_chunk' in chunk_path])\n",
    "\n",
    "    print_f('chunks', chunk_paths)\n",
    "\n",
    "    if len(chunk_paths) > 0:\n",
    "        for i, chunk_path in enumerate(chunk_paths):\n",
    "            chunk = torch.load(chunk_path)\n",
    "\n",
    "            skip_n_articles += len(chunk)\n",
    "            print_f(f'Chunk at \"{chunk_path}\" has {len(chunk)} articles.')\n",
    "\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "\n",
    "        print_f('skip:', skip_n_articles)\n",
    "\n",
    "        if skip_n_articles >= len(dataset):\n",
    "            print_f('Looks like the dataset if fully embedded already. Skipping this dataset...')\n",
    "            continue\n",
    "\n",
    "        print_f('dataset original', len(dataset))\n",
    "\n",
    "        dataset.input_ids = dataset.input_ids[skip_n_articles:]\n",
    "        dataset.attention_mask = dataset.attention_mask[skip_n_articles:]\n",
    "        dataset.labels = dataset.labels[skip_n_articles:]\n",
    "\n",
    "        print_f('dataset after skipping', len(dataset))\n",
    "\n",
    "    iterator = DataLoader(dataset, batch_size=batch_size)\n",
    "    print_f('Vectorizing dataset for ', output_path)\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    chunk_id = len(chunk_paths) + 1\n",
    "\n",
    "    print_f('Starting at chunk id', chunk_id)\n",
    "\n",
    "    for i, batch in enumerate(tqdm(iterator)):\n",
    "        inputs, attention_mask, labels = batch\n",
    "\n",
    "        real_batch_size = inputs.shape[0]\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=inputs, attention_mask=attention_mask)\n",
    "\n",
    "        output = output[0]\n",
    "\n",
    "        # indices of last non-padded elements in each sequence\n",
    "        # adopted from https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt2/modeling_gpt2.py#L1290-L1302\n",
    "        last_non_padded_ids = torch.ne(inputs, test_dataset.tokenizer.pad_token_id).sum(-1) - 1\n",
    "\n",
    "        embeddings = output[range(real_batch_size), last_non_padded_ids, :]\n",
    "\n",
    "        X_train += embeddings.detach().cpu()\n",
    "        y_train += labels.detach().cpu()\n",
    "\n",
    "        if len(X_train) >= chunk_size:\n",
    "            print_f('Saving chunk:', output_path)\n",
    "            saved_dataset = GPTVectorizedDataset(torch.stack(X_train), torch.stack(y_train))\n",
    "            torch.save(saved_dataset, output_path, pickle_protocol=4)\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            chunk_id += 1\n",
    "\n",
    "    # take care of what's left after loop\n",
    "    if len(X_train) >= 0:\n",
    "        print_f('Saving chunk:', output_path)\n",
    "        saved_dataset = GPTVectorizedDataset(torch.stack(X_train), torch.stack(y_train))\n",
    "        torch.save(saved_dataset, output_path, pickle_protocol=4)\n",
    "\n",
    "print_f('All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
