{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports seem good!\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = \"/gpfs/space/home/roosild/ut-mit-news-classify/NYT/\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from utils import print_f, load_nyt_articles, split_to_n_chunks\n",
    "import gc\n",
    "\n",
    "print_f('All imports seem good!')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print_f('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################\n",
    "# LOAD TRAIN DATA\n",
    "########################\n",
    "\n",
    "TOTAL_NR_OF_CHUNKS = 4\n",
    "\n",
    "## change this for each chunk\n",
    "NR = 4\n",
    "\n",
    "## change this for non-cutoff data\n",
    "cutoff_end_chars = True \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NYT train dataset...\n",
      "Data loaded from /gpfs/space/projects/stud_nlp_share/data/NYTcorpus_train.p.gz\n",
      "Articles after filtering: 1195938\n",
      "Splitting to 4 chunks and processing chunk nr 4\n",
      "There are 298986 articles in this chunk\n",
      "Will save train to: /gpfs/space/projects/stud_nlp_share/cutoff/GPT/tokenized/train_1195k_min500_cutoff_replace_chunk4of4.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_f('Loading NYT train dataset...')\n",
    "\n",
    "train_articles, _ = load_nyt_articles('/gpfs/space/projects/stud_nlp_share/data/NYTcorpus_train.p.gz', min_len=500, cutoff_tags=cutoff_end_chars)\n",
    "\n",
    "print_f(f'Splitting to {TOTAL_NR_OF_CHUNKS} chunks and processing chunk nr {NR}')\n",
    "\n",
    "size_str = f'{len(train_articles)//1000}k'\n",
    "\n",
    "train_articles = split_to_n_chunks(train_articles, TOTAL_NR_OF_CHUNKS)[NR-1]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print_f(f'There are {len(train_articles)} articles in this chunk')\n",
    "\n",
    "if cutoff_end_chars:\n",
    "    ending = 'min500_cutoff_replace'\n",
    "else:\n",
    "    ending = 'min500_complete'\n",
    "\n",
    "os.makedirs('tokenized', exist_ok=True)\n",
    "\n",
    "train_path = f'/gpfs/space/projects/stud_nlp_share/cutoff/GPT/tokenized/train_{size_str}_{ending}_chunk{NR}of{TOTAL_NR_OF_CHUNKS}.pt'\n",
    "\n",
    "print('Will save train to:', train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################\n",
    "# TOKENIZE DATA\n",
    "########################\n",
    "from utils import GPTTokenizedDatasetWithoutLabels, load_nyt_data\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "MODEL = 'gpt2'\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset...\n",
      "GPTTokenizedDataset init - tokenizing...\n",
      "GPTTokenizedDataset init - tokenizing done.\n",
      "Saving tokenized dataset...\n",
      "Done train tokenization part in 1668.3802 seconds!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    print_f('Tokenizing train dataset...')\n",
    "    train_dataset = GPTTokenizedDatasetWithoutLabels(train_articles, tokenizer)\n",
    "    del train_articles\n",
    "    gc.collect()\n",
    "    \n",
    "    print_f('Saving tokenized dataset...')\n",
    "    torch.save(train_dataset, train_path, pickle_protocol=4)\n",
    "    \n",
    "    del train_dataset\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    print_f('Found tokenized training set...')\n",
    "    train_dataset = torch.load(train_path)\n",
    "\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print_f(f'Done train tokenization part in {toc - tic:0.4f} seconds!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NYT test dataset...\n",
      "Data loaded from /gpfs/space/projects/stud_nlp_share/data/NYTcorpus_test.p.gz\n",
      "Articles after filtering: 133032\n",
      "Splitting to 4 chunks and processing chunk nr 4\n",
      "There are 33258 articles in this chunk\n",
      "Will save test to: /gpfs/space/projects/stud_nlp_share/cutoff/GPT/tokenized/test_133k_min500_cutoff_replace_chunk4of4.pt\n"
     ]
    }
   ],
   "source": [
    "print_f('Loading NYT test dataset...')\n",
    "\n",
    "test_articles, _ = load_nyt_articles('/gpfs/space/projects/stud_nlp_share/data/NYTcorpus_test.p.gz', min_len=500, cutoff_tags=cutoff_end_chars)\n",
    "\n",
    "\n",
    "print_f(f'Splitting to {TOTAL_NR_OF_CHUNKS} chunks and processing chunk nr {NR}')\n",
    "\n",
    "size_str = f'{len(test_articles)//1000}k'\n",
    "\n",
    "test_articles = split_to_n_chunks(test_articles, TOTAL_NR_OF_CHUNKS)[NR-1]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print_f(f'There are {len(test_articles)} articles in this chunk')\n",
    "\n",
    "if cutoff_end_chars:\n",
    "    ending = 'min500_cutoff_replace'\n",
    "else:\n",
    "    ending = 'min500_complete'\n",
    "\n",
    "os.makedirs('tokenized', exist_ok=True)\n",
    "\n",
    "test_path = f'/gpfs/space/projects/stud_nlp_share/cutoff/GPT/tokenized/test_{size_str}_{ending}_chunk{NR}of{TOTAL_NR_OF_CHUNKS}.pt'\n",
    "\n",
    "print('Will save test to:', test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test dataset...\n",
      "GPTTokenizedDataset init - tokenizing...\n",
      "GPTTokenizedDataset init - tokenizing done.\n",
      "Saving tokenized dataset...\n",
      "Done test tokenization part in 100.4735 seconds!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    print_f('Tokenizing test dataset...')\n",
    "    test_dataset = GPTTokenizedDatasetWithoutLabels(test_articles, tokenizer)\n",
    "    del test_articles\n",
    "    gc.collect()\n",
    "    \n",
    "    print_f('Saving tokenized dataset...')\n",
    "    torch.save(test_dataset, test_path, pickle_protocol=4)\n",
    "    del test_dataset\n",
    "    gc.collect()\n",
    "else:\n",
    "    print_f('Found tokenized training set...')\n",
    "    test_dataset = torch.load(test_path)\n",
    "\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print_f(f'Done test tokenization part in {toc - tic:0.4f} seconds!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-conda-kernel",
   "language": "python",
   "name": "nlp-conda-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
